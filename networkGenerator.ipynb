{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to analysis gps data from Event 131 table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Last Run Initiated: 2018-10-19 20:49:35.454170\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    div.output_area{\n",
       "        max-height:10000px;\n",
       "        overflow:scroll;\n",
       "    }\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime as dt\n",
    "start = dt.datetime.now()\n",
    "print(\"Notebook Last Run Initiated: \"+str(start))\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"\"\"<style>\n",
    "    div.output_area{\n",
    "        max-height:10000px;\n",
    "        overflow:scroll;\n",
    "    }\n",
    "</style>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Locally or on Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "isLocal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:49:38 for Begin\n",
      "3.6.4 |Anaconda custom (64-bit)| (default, Jan 16 2018, 18:10:19) \n",
      "[GCC 7.2.0]\n",
      "0.7.1\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\",5000)\n",
    "pd.set_option(\"display.max_columns\",200)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math, random\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, sum, udf, countDistinct, col, datediff, max, min, stddev, count, skewness, asc\n",
    "from pyspark.sql.functions import kurtosis, corr, hour, date_format, desc, sqrt, weekofyear, month, dayofyear, pow\n",
    "from pyspark.sql.types import IntegerType, BooleanType, DateType, StringType, LongType, Row\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists,join\n",
    "from haversine import haversine\n",
    "import igraph as ig\n",
    "import pygeohash as geohash\n",
    "import sys\n",
    "from functools import reduce\n",
    "\n",
    "from networkTrips import loadDeviceTrips, saveDeviceTrips\n",
    "from sparkUtils import getSparkDFDim, modFunc, castLong, fillNAWithZero, castDate, getPandasDataFrame, castString, commonSparkAddOns\n",
    "from sparkUtils import setupSpark, setupHive, setupSQL, getHiveData, isSparkDataFrame\n",
    "from modelio import saveData, loadData, saveSparkData, appendSparkData, saveJoblib, loadJoblib\n",
    "from timeUtils import clock, elapsed, getTimeSuffix, getDateTime, addDays, printDateTime, getFirstLastDay\n",
    "from pandasUtils import castDateTime, castInt64, cutDataFrameByDate, convertToDate, isSeries, isDataFrame, getColData\n",
    "from network import getNetworkGraph, rows_to_pandas_df, pandas_df_to_rows, extract_features\n",
    "\n",
    "clock()\n",
    "print(sys.version)\n",
    "print(ig.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark & Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:49:38 for Setting up Spark/PySpark\n",
      "PySpark home is /nas/isg_prodops_work/astro_research/spark-2.2.1-bin-hadoop2.6\n",
      "Current Time is Fri Oct 19, 2018 20:50:11 for Done with Setting up Spark/PySpark\n",
      "Process [Done with Setting up Spark/PySpark] took 33 seconds.\n",
      "  Adding common files to SparkContent\n",
      "  All common files added to SparkContent\n",
      "Current Time is Fri Oct 19, 2018 20:50:16 for Finished Spark Setup\n"
     ]
    }
   ],
   "source": [
    "if isLocal is False:\n",
    "    sc     = setupSpark()\n",
    "    if sc is not None:\n",
    "        commonSparkAddOns(sc, debug=True)\n",
    "        hc     = setupHive(sc)\n",
    "        scsql  = setupSQL(sc)\n",
    "start, cmt = clock(\"Finished Spark Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment GPS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:50:16 for Last Run\n"
     ]
    }
   ],
   "source": [
    "if not isLocal:\n",
    "    from pyspark.sql.functions import udf\n",
    "    from pyspark.sql.types import StringType, DoubleType, FloatType\n",
    "\n",
    "    def getGeo3(lat, long):\n",
    "        try:\n",
    "            retval = geohash.encode(lat, long, precision=3)\n",
    "        except:\n",
    "            retval = None\n",
    "        return retval\n",
    "\n",
    "    def getGeo4(lat, long):\n",
    "        try:\n",
    "            retval = geohash.encode(lat, long, precision=4)\n",
    "        except:\n",
    "            retval = None\n",
    "        return retval\n",
    "\n",
    "    def getGeo5(lat, long):\n",
    "        try:\n",
    "            retval = geohash.encode(lat, long, precision=5)\n",
    "        except:\n",
    "            retval = None\n",
    "        return retval\n",
    "\n",
    "    def getGeo6(lat, long):\n",
    "        try:\n",
    "            retval = geohash.encode(lat, long, precision=6)\n",
    "        except:\n",
    "            retval = None\n",
    "        return retval\n",
    "\n",
    "    def getGeo7(lat, long):\n",
    "        try:\n",
    "            retval = geohash.encode(lat, long, precision=7)\n",
    "        except:\n",
    "            retval = None\n",
    "        return retval\n",
    "\n",
    "    def getGeo8(lat, long):\n",
    "        try:\n",
    "            retval = geohash.encode(lat, long, precision=8)\n",
    "        except:\n",
    "            retval = None\n",
    "        return retval\n",
    "\n",
    "    prec=\"\"\n",
    "    get_geo3_udf = udf(lambda lat,long: getGeo3(lat, long), StringType())\n",
    "    get_geo4_udf = udf(lambda lat,long: getGeo4(lat, long), StringType())\n",
    "    get_geo5_udf = udf(lambda lat,long: getGeo5(lat, long), StringType())\n",
    "    get_geo6_udf = udf(lambda lat,long: getGeo6(lat, long), StringType())\n",
    "    get_geo7_udf = udf(lambda lat,long: getGeo7(lat, long), StringType())\n",
    "    get_geo8_udf = udf(lambda lat,long: getGeo8(lat, long), StringType())\n",
    "        \n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:50:16 for Last Run\n"
     ]
    }
   ],
   "source": [
    "def loadGeoData(suffix, prec=7):\n",
    "    from modelio import loadJoblib\n",
    "    from pickle import load\n",
    "    from timeUtils import clock, elapsed\n",
    "    startall,cmtall = clock(\"Loading Geo Data\")\n",
    "    geomapname = \"/home/tgadf/geomap-{0}-{1}.p\".format(prec, suffix)\n",
    "    start, cmt = clock(\"Loading {0}\".format(geomapname))\n",
    "    geomap     = load(open(geomapname, \"rb\"))\n",
    "    elapsed(start, cmt)\n",
    "    return geomap\n",
    "\n",
    "def castInt(coldata):\n",
    "    import pandas as pd\n",
    "    coldata = pd.to_numeric(coldata, downcast='signed', errors='coerce')\n",
    "    coldata = coldata.fillna(0)\n",
    "    return coldata\n",
    "\n",
    "def createGeoDataFrame(geomap, prec=7):\n",
    "    from timeUtils import clock, elapsed\n",
    "    from pandas import DataFrame\n",
    "    start, cmt = clock(\"Creating Pandas Data Frame\")\n",
    "    geodf = DataFrame(geomap).T\n",
    "    geodf.reset_index(inplace=True)\n",
    "    cols = list(geodf.columns)\n",
    "    cols[0] = \"Geo{0}\".format(prec)\n",
    "    geodf.columns = cols\n",
    "\n",
    "    for col in geodf.columns:\n",
    "        geodf[col] = geodf[col].astype(str)\n",
    "        if col.startswith(\"Geo\") is False:\n",
    "            geodf[col] = castInt(geodf[col])\n",
    "        \n",
    "    elapsed(start, cmt)\n",
    "    return geodf\n",
    "\n",
    "def createGeohashPandasDataFrame(suffix, prec=7):\n",
    "    geomap = loadGeoData(suffix, prec)\n",
    "    geodf  = createGeoDataFrame(geomap)\n",
    "    return geodf\n",
    "\n",
    "def createGeohashSparkDataFrame(suffix, prec, scsql, inputDBName):    \n",
    "    from timeUtils import clock, elapsed\n",
    "    startall,cmtall = clock(\"Creating Spark DataFrame from {0}\".format(suffix))\n",
    "    pddf = createGeohashPandasDataFrame(suffix, prec)\n",
    "    \n",
    "    start, cmt = clock(\"Creating Spark DataFrame\")\n",
    "    spdf   = scsql.createDataFrame(pddf)\n",
    "    elapsed(start, cmt)\n",
    "\n",
    "    start, cmt = clock(\"Saving spark dataframe to HIVE\")\n",
    "    saveSparkData(spdf, inputDBName, \"Geo{0}Map\".format(suffix))\n",
    "    elapsed(startall, cmtall)\n",
    "    \n",
    "def getGeohashSparkDataFrame(suffix, prec, hc, scsql, inputDBName, force=False):\n",
    "    if force:\n",
    "        createGeohashSparkDataFrame(suffix, prec, scsql, inputDBName)\n",
    "        spdf = None\n",
    "    else:\n",
    "        try:\n",
    "            spdf = getHiveData(hc, inputDBName, \"Geo{0}Map\".format(suffix))\n",
    "            spdf.cache()\n",
    "            print(\"Total Rows = {0}\".format(spdf.count()))\n",
    "            print(\"   Columns = {0}\".format(spdf.columns))\n",
    "        except:\n",
    "            createGeohashSparkDataFrame(suffix, prec, scsql, inputDBName)\n",
    "            spdf = None\n",
    "    return spdf\n",
    "\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:50:16 for Last Run\n"
     ]
    }
   ],
   "source": [
    "def castDouble(spdf, colname):\n",
    "    from pyspark.sql.types import DoubleType\n",
    "    spdf = spdf.withColumn(colname, spdf[colname].cast(DoubleType()))\n",
    "    return spdf\n",
    "\n",
    "def castTimestamp(spdf, colname):\n",
    "    from pyspark.sql.types import TimestampType\n",
    "    spdf = spdf.withColumn(colname, spdf[colname].cast(TimestampType()))\n",
    "    return spdf\n",
    "\n",
    "def DQ(spdf, spdfname = \"DQ\"):\n",
    "    from pyspark.sql.functions import countDistinct, col, max, min\n",
    "    print(\"Showing Data Quality for {0}\".format(spdfname))\n",
    "    start = clock()\n",
    "    print(\"Total Rows = {0}\".format(spdf.count()))\n",
    "    spdf.agg(countDistinct(\"device\")).show()\n",
    "    spdf.agg(min(col(\"start\"))).show()\n",
    "    spdf.agg(max(col(\"end\"))).show()\n",
    "    elapsed(start, comment=\"Data Quality\")\n",
    "    \n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:50:16 for Setting Params\n",
      "Current Time is Fri Oct 19, 2018 20:50:16 for Done with Setting Params\n",
      "Process [Done with Setting Params] took 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "start, cmt = clock(\"Setting Params\")\n",
    "from timeUtils import getTimeSuffix\n",
    "outdatadir     = '/home/tgadf/astro_research_data/futuremiles/sparkData'\n",
    "\n",
    "inputDBName        = 'dra_cc_ce_res'\n",
    "outputDBName       = inputDBName\n",
    "gpsTableName       = 'gps_trip_data'\n",
    "gpsTableName       = 'r4henricheddata2018Not'\n",
    "networkTableName   = 'device_network_data'\n",
    "gpsData            = None\n",
    "\n",
    "elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The GPS Trip Endpoint Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Thu Oct 18, 2018 19:50:14 for Getting data\n",
      "Current Time is Thu Oct 18, 2018 19:50:14 for Done with Getting data\n",
      "Process [Done with Getting data] took 0 seconds.\n"
     ]
    }
   ],
   "source": [
    "startdata,cmtdata = clock(\"Getting data\")\n",
    "\n",
    "#gpsData=None    \n",
    "if isLocal is False:\n",
    "    if gpsData is None:\n",
    "        start, cmt = clock(comment=\"Getting GSP Trip Endpoint Data\")\n",
    "        gpsData = getHiveData(hc, inputDBName, gpsTableName)\n",
    "        gpsData = castDouble(gpsData, 'lat0')\n",
    "        gpsData = castDouble(gpsData, 'lat1')\n",
    "        gpsData = castDouble(gpsData, 'long0')\n",
    "        gpsData = castDouble(gpsData, 'long1')\n",
    "        elapsed(start, cmt)\n",
    "        start, cmt = clock(\"Getting GPS Table Count\")\n",
    "        print(\"There are {0} entries in the GPS table\".format(gpsData.count()))\n",
    "        elapsed(start, cmt)\n",
    "    \n",
    "        DQ(gpsData)\n",
    "        tmpData = gpsData.limit(1000)\n",
    "        #tmpData.show()\n",
    "        \n",
    "elapsed(startdata,cmtdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoHEREMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"HEREPOI\"\n",
    "    spGeoHEREMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spGeoHEREMap):\n",
    "        oldColumns = [x for x in list(spGeoHEREMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns ]\n",
    "        spGeoHEREMap = reduce(lambda spGeoHEREMap, idx: spGeoHEREMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoHEREMap)\n",
    "        print(\"   Columns = {0}\".format(spGeoHEREMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illinois Specific Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=False\n",
    "    suffix = \"InterstateIllinois\"\n",
    "    spGeoInterstateIllinoisMap = getGeohashSparkDataFrame(suffix, 8, hc, scsql, inputDBName, force=force)\n",
    "    \n",
    "    suffix = \"USRteIllinois\"\n",
    "    spGeoUSRteIllinoisMap = getGeohashSparkDataFrame(suffix, 8, hc, scsql, inputDBName, force=force)\n",
    "    \n",
    "    suffix = \"StateRteIllinois\"\n",
    "    spGeoStateRteIllinoisMap = getGeohashSparkDataFrame(suffix, 8, hc, scsql, inputDBName, force=force)\n",
    "    \n",
    "    suffix = \"HighwayIllinois\"\n",
    "    spGeoHighwayIllinoisMap = getGeohashSparkDataFrame(suffix, 8, hc, scsql, inputDBName, force=force)\n",
    "    \n",
    "    suffix = \"MajorRdIllinois\"\n",
    "    spGeoMajorRdIllinoisMap = getGeohashSparkDataFrame(suffix, 8, hc, scsql, inputDBName, force=force)\n",
    "    \n",
    "    suffix = \"RoadIllinois\"\n",
    "    spGeoRoadIllinoisMap = getGeohashSparkDataFrame(suffix, 8, hc, scsql, inputDBName, force=force)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interstate Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=False\n",
    "    suffix = \"InterstateA\"\n",
    "    spGeoRoadsAMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"InterstateB\"\n",
    "    spGeoRoadsBMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"InterstateC\"\n",
    "    spGeoRoadsCMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "\n",
    "    from functools import reduce  # For Python 3.x\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def unionAll(*dfs):\n",
    "        return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "    if all(isSparkDataFrame(x) for x in [spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap]):\n",
    "        suffix=\"ROADS\"\n",
    "        spGeoInterstateMap = unionAll(spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap)\n",
    "        oldColumns = [x for x in list(spGeoInterstateMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spGeoInterstateMap = reduce(lambda spGeoInterstateMap, idx: spGeoInterstateMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoInterstateMap)        \n",
    "        print(\"Total Rows = {0}\".format(spGeoInterstateMap.count()))    \n",
    "        print(\"   Columns = {0}\".format(spGeoInterstateMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USRte Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=False\n",
    "    suffix = \"USRteA\"\n",
    "    spGeoRoadsAMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"USRteB\"\n",
    "    spGeoRoadsBMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"USRteC\"\n",
    "    spGeoRoadsCMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "\n",
    "    from functools import reduce  # For Python 3.x\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def unionAll(*dfs):\n",
    "        return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "    if all(isSparkDataFrame(x) for x in [spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap]):\n",
    "        spGeoUSRteMap = unionAll(spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap)\n",
    "        suffix=\"ROADS\"\n",
    "        oldColumns = [x for x in list(spGeoUSRteMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spGeoUSRteMap = reduce(lambda spGeoUSRteMap, idx: spGeoUSRteMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoUSRteMap)        \n",
    "        print(\"Total Rows = {0}\".format(spGeoUSRteMap.count()))    \n",
    "        print(\"   Columns = {0}\".format(spGeoUSRteMap.columns))        \n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StateRte Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=False\n",
    "    suffix = \"StateRteA\"\n",
    "    spGeoRoadsAMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"StateRteB\"\n",
    "    spGeoRoadsBMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"StateRteC\"\n",
    "    spGeoRoadsCMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "\n",
    "    from functools import reduce  # For Python 3.x\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def unionAll(*dfs):\n",
    "        return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "    if all(isSparkDataFrame(x) for x in [spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap]):\n",
    "        spGeoStateRteMap = unionAll(spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap)\n",
    "        suffix=\"ROADS\"\n",
    "        oldColumns = [x for x in list(spGeoStateRteMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spGeoStateRteMap = reduce(lambda spGeoStateRteMap, idx: spGeoStateRteMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoStateRteMap)        \n",
    "        print(\"Total Rows = {0}\".format(spGeoStateRteMap.count()))    \n",
    "        print(\"   Columns = {0}\".format(spGeoStateRteMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highway Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=False\n",
    "    suffix = \"HighwayA\"\n",
    "    spGeoRoadsAMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"HighwayB\"\n",
    "    spGeoRoadsBMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"HighwayC\"\n",
    "    spGeoRoadsCMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "\n",
    "    from functools import reduce  # For Python 3.x\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def unionAll(*dfs):\n",
    "        return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "    if all(isSparkDataFrame(x) for x in [spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap]):\n",
    "        spGeoHighwayMap = unionAll(spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap)\n",
    "        suffix=\"ROADS\"\n",
    "        oldColumns = [x for x in list(spGeoHighwayMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spGeoHighwayMap = reduce(lambda spGeoHighwayMap, idx: spGeoHighwayMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoHighwayMap)        \n",
    "        print(\"Total Rows = {0}\".format(spGeoHighwayMap.count()))    \n",
    "        print(\"   Columns = {0}\".format(spGeoHighwayMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MajorRd Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=False\n",
    "    suffix = \"MajorRdA\"\n",
    "    spGeoRoadsAMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"MajorRdB\"\n",
    "    spGeoRoadsBMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"MajorRdC\"\n",
    "    spGeoRoadsCMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "\n",
    "    from functools import reduce  # For Python 3.x\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def unionAll(*dfs):\n",
    "        return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "    if all(isSparkDataFrame(x) for x in [spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap]):\n",
    "        spGeoMajorRdMap = unionAll(spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap)\n",
    "        suffix=\"ROADS\"\n",
    "        oldColumns = [x for x in list(spGeoMajorRdMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        newColumns = [x.replace(\"rd\", \"Rd\") for x in newColumns]\n",
    "        spGeoMajorRdMap = reduce(lambda spGeoMajorRdMap, idx: spGeoMajorRdMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoMajorRdMap) \n",
    "        print(\"Total Rows = {0}\".format(spGeoMajorRdMap.count()))    \n",
    "        print(\"   Columns = {0}\".format(spGeoMajorRdMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic Road Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRoadsMap = None\n",
    "if not isLocal:\n",
    "    force=True\n",
    "    suffix = \"RoadA\"\n",
    "    spGeoRoadsAMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"RoadB\"\n",
    "    spGeoRoadsBMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "    suffix = \"RoadC\"\n",
    "    spGeoRoadsCMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=force)\n",
    "\n",
    "    from functools import reduce  # For Python 3.x\n",
    "    from pyspark.sql import DataFrame\n",
    "\n",
    "    def unionAll(*dfs):\n",
    "        return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "    if all(isSparkDataFrame(x) for x in [spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap]):\n",
    "        spGeoGenRdMap = unionAll(spGeoRoadsAMap, spGeoRoadsBMap, spGeoRoadsCMap)\n",
    "        suffix=\"ROADS\"\n",
    "        oldColumns = [x for x in list(spGeoGenRdMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        newColumns = [x.replace(\"rd\", \"Rd\") for x in newColumns]\n",
    "        spGeoGenRdMap = reduce(lambda spGeoGenRdMap, idx: spGeoGenRdMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoGenRdMap) \n",
    "        print(\"Total Rows = {0}\".format(spGeoGenRdMap.count()))    \n",
    "        print(\"   Columns = {0}\".format(spGeoGenRdMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drivewise POI Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoPOIMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"POI\"\n",
    "    spGeoPOIMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spGeoPOIMap):\n",
    "        spGeoPOIMap = spGeoPOIMap.withColumnRenamed('geo7', 'Geo7')\n",
    "        spGeoPOIMap = spGeoPOIMap.withColumnRenamed('Count', 'POIVisits')\n",
    "        spGeoPOIMap = spGeoPOIMap.withColumnRenamed('Distinct', 'POIUniqueVisits')    \n",
    "        print(\"   Columns = {0}\".format(spGeoPOIMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic OSM Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spOSMTrafficMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"trafficOSM\"\n",
    "    spOSMTrafficMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spOSMTrafficMap):\n",
    "        suffix=\"OSM\"\n",
    "        oldColumns = [x for x in list(spOSMTrafficMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spOSMTrafficMap = reduce(lambda spOSMTrafficMap, idx: spOSMTrafficMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spOSMTrafficMap)  \n",
    "        print(\"   Columns = {0}\".format(spOSMTrafficMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Place of Worship (POFW) OSM Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spOSMPofwMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"pofwOSM\"\n",
    "    spOSMPofwMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spOSMPofwMap):\n",
    "        suffix=\"OSM\"\n",
    "        oldColumns = [x for x in list(spOSMPofwMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spOSMPofwMap = reduce(lambda spOSMPofwMap, idx: spOSMPofwMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spOSMPofwMap)  \n",
    "        print(\"   Columns = {0}\".format(spOSMPofwMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transport OSM Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spOSMTransportMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"transportOSM\"\n",
    "    spOSMTransportMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spOSMTransportMap):\n",
    "        suffix=\"OSM\"\n",
    "        oldColumns = [x for x in list(spOSMTransportMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spOSMTransportMap = reduce(lambda spOSMTransportMap, idx: spOSMTransportMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spOSMTransportMap)  \n",
    "        print(\"   Columns = {0}\".format(spOSMTransportMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POI OSM Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spOSMPOIMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"poisOSM\"\n",
    "    spOSMPOIMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spOSMPOIMap):\n",
    "        suffix=\"OSM\"\n",
    "        oldColumns = [x for x in list(spOSMPOIMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spOSMPOIMap = reduce(lambda spOSMPOIMap, idx: spOSMPOIMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spOSMPOIMap)  \n",
    "        print(\"   Columns = {0}\".format(spOSMPOIMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terminal Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoTerminalsMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"Terminals\"\n",
    "    spGeoTerminalsMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spGeoTerminalsMap):\n",
    "        oldColumns = [x for x in list(spGeoTerminalsMap.columns) if x != \"Geo7\"]\n",
    "        newColumns = [\"{0}{1}\".format(suffix, x.title()) for x in oldColumns]\n",
    "        spGeoTerminalsMap = reduce(lambda spGeoTerminalsMap, idx: spGeoTerminalsMap.withColumnRenamed(oldColumns[idx], newColumns[idx]), range(len(oldColumns)), spGeoTerminalsMap)  \n",
    "        print(\"   Columns = {0}\".format(spGeoTerminalsMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Fri Oct 19, 2018 20:50:16 for Creating Spark DataFrame from CouSub\n",
      "Current Time is Fri Oct 19, 2018 20:50:16 for Loading Geo Data\n",
      "Current Time is Fri Oct 19, 2018 20:50:16 for Loading /home/tgadf/geomap-6-CouSub.p\n",
      "Current Time is Fri Oct 19, 2018 20:50:39 for Done with Loading /home/tgadf/geomap-6-CouSub.p\n",
      "Process [Done with Loading /home/tgadf/geomap-6-CouSub.p] took 22 seconds.\n",
      "Current Time is Fri Oct 19, 2018 20:50:39 for Creating Pandas Data Frame\n"
     ]
    }
   ],
   "source": [
    "spGeoCensusCSAMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"CouSub\"\n",
    "    spGeoCensusCSAMap = getGeohashSparkDataFrame(suffix, 6, hc, scsql, inputDBName, force=True)\n",
    "    if isSparkDataFrame(spGeoCensusCSAMap):\n",
    "        print(\"   Columns = {0}\".format(spGeoCensusCSAMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Time is Thu Oct 18, 2018 20:35:05 for Creating Spark DataFrame from CouSub\n",
      "Current Time is Thu Oct 18, 2018 20:35:05 for Loading Geo Data\n",
      "Current Time is Thu Oct 18, 2018 20:35:05 for Loading /home/tgadf/geomap-6-CouSub.p\n",
      "Current Time is Thu Oct 18, 2018 20:35:24 for Done with Loading /home/tgadf/geomap-6-CouSub.p\n",
      "Process [Done with Loading /home/tgadf/geomap-6-CouSub.p] took 19 seconds.\n",
      "Current Time is Thu Oct 18, 2018 20:35:24 for Creating Pandas Data Frame\n",
      "Current Time is Thu Oct 18, 2018 20:58:06 for Done with Creating Pandas Data Frame\n",
      "Process [Done with Creating Pandas Data Frame] took 22.7 minutes.\n",
      "Current Time is Thu Oct 18, 2018 20:58:09 for Creating Spark DataFrame\n",
      "Current Time is Thu Oct 18, 2018 21:07:37 for Done with Creating Spark DataFrame\n",
      "Process [Done with Creating Spark DataFrame] took 9.4 minutes.\n",
      "Current Time is Thu Oct 18, 2018 21:07:37 for Saving spark dataframe to HIVE\n",
      "Current Time is Thu Oct 18, 2018 21:07:37 for Saving spark dataframe to dra_cc_ce_res.GeoCouSubMap\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'write'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-2536317aa4b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspGeoCensusMap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetGeohashSparkDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CouSub\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscsql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDBName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspGeoCensusMap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#geomap = loadGeoData(\"CSA_2010Census_DP1\", 6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#createGeohashPandasDataFrame(\"CSA_2010Census_DP1\", 6)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#geomap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1a6f48773f77>\u001b[0m in \u001b[0;36mgetGeohashSparkDataFrame\u001b[0;34m(suffix, prec, hc, scsql, inputDBName, force)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetGeohashSparkDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscsql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDBName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mcreateGeohashSparkDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscsql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDBName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mspdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1a6f48773f77>\u001b[0m in \u001b[0;36mcreateGeohashSparkDataFrame\u001b[0;34m(suffix, prec, scsql, inputDBName)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving spark dataframe to HIVE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0msaveSparkData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputDBName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Geo{0}Map\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0melapsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmtall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nas/isg_prodops_work/tgadf/astro/astro_research/PatternsOfLife/modelio.py\u001b[0m in \u001b[0;36msaveSparkData\u001b[0;34m(spdf, dbname, tablename)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msaveSparkData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving spark dataframe to {0}.{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0mspdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0}.{1}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m     \u001b[0melapsed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'write'"
     ]
    }
   ],
   "source": [
    "spGeoCensusMap = getGeohashSparkDataFrame(\"\", 6, hc, scsql, inputDBName, force=True)\n",
    "jData = spGeoCensusMap\n",
    "#geomap = loadGeoData(\"CSA_2010Census_DP1\", 6)\n",
    "#createGeohashPandasDataFrame(\"CSA_2010Census_DP1\", 6)\n",
    "#geomap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------+-------------------+-----------------+-------------------+----+----+-----+------+----+----+----+----+\n",
      "|         device|            lat0|              long0|             lat1|              long1|geo3|geo4| geo5|  geo6|CSA3|CSA4|CSA5|CSA6|\n",
      "+---------------+----------------+-------------------+-----------------+-------------------+----+----+-----+------+----+----+----+----+\n",
      "|000V8N8MBGFQQ20|33.4540082216686|-112.07394435906116|33.49648051732159|-112.06978008883728| 9tb|9tbq|9tbq3|9tbq3d|null|null|null|null|\n",
      "+---------------+----------------+-------------------+-----------------+-------------------+----+----+-----+------+----+----+----+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xData = tmpData.drop('GeoPoint', 'Accepted', 'summary', 'Policy_Identifier', 'ADW_Enroll_ID', 'ADW_Member_ID', 'ADW_Operator_ID', 'CCT_Enroll_ID', 'CCT_Member_ID', 'CCT_Operator_ID', 'tripId', 'tripStartLocation', 'tripEndLocation', 'tripTerminateReason', 'tripRejectReason', 'mobileAppDevice', 'mobileAppVersion', 'mobileOsVersion')\n",
    "xData = xData.drop('Start', 'End', 'Duration', 'total_miles', 'inR4HTimePeriod')\n",
    "xData = xData.withColumn('geo3', get_geo3_udf('lat0', 'long0'))\n",
    "xData = xData.withColumn('geo4', get_geo4_udf('lat0', 'long0'))\n",
    "xData = xData.withColumn('geo5', get_geo5_udf('lat0', 'long0'))\n",
    "xData = xData.withColumn('geo6', get_geo6_udf('lat0', 'long0'))\n",
    "xData = xData.join(jData, on=[xData['geo3'] == jData.Geo], how=\"left\").drop(\"Geo\").withColumnRenamed(\"CSA\", \"CSA3\")\n",
    "xData = xData.join(jData, on=[xData['geo4'] == jData.Geo], how=\"left\").drop(\"Geo\").withColumnRenamed(\"CSA\", \"CSA4\")\n",
    "xData = xData.join(jData, on=[xData['geo5'] == jData.Geo], how=\"left\").drop(\"Geo\").withColumnRenamed(\"CSA\", \"CSA5\")\n",
    "xData = xData.join(jData, on=[xData['geo6'] == jData.Geo], how=\"left\").drop(\"Geo\").withColumnRenamed(\"CSA\", \"CSA6\")\n",
    "xData.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "geos = [x[0] for x in jData.select('Geo').collect()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+-------------------+-----------------+-------------------+----+----+-----+------+----+----+\n",
      "|         device|             lat0|              long0|             lat1|              long1|geo3|geo4| geo5|  geo6| Geo| CSA|\n",
      "+---------------+-----------------+-------------------+-----------------+-------------------+----+----+-----+------+----+----+\n",
      "|000V8N8MBGFQQ20|33.48029410807537|-112.05960016294972|33.48713300373057|-111.91922504383615| 9tb|9tbq|9tbq6|9tbq6n|null|null|\n",
      "+---------------+-----------------+-------------------+-----------------+-------------------+----+----+-----+------+----+----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 351840), (5, 36629), (4, 1289), (3, 1), (2, 1)]\n",
      "419\n",
      "52\n"
     ]
    }
   ],
   "source": [
    "#jData.Geo\n",
    "geos = [x[0] for x in jData.select('Geo').collect()]\n",
    "from collections import Counter\n",
    "tmp = Counter()\n",
    "for geo in geos:\n",
    "    tmp[len(geo)] += 1\n",
    "print(tmp.most_common())\n",
    "#[i for i in jData.Geo.collect()]\n",
    "\n",
    "#tmpData\n",
    "\n",
    "xData = tmpData.withColumn('geo{0}0'.format(5), get_geo5_udf('lat0', 'long0'))\n",
    "x = dropNullGeoJoin(xData.join(jData, on=[xData.geo50 == jData.Geo], how=\"left\"), \"Geo\").drop('geo{0}0'.format(5))\n",
    "geo5 = [y[0] for y in x.select('Geo').collect()]\n",
    "xData = tmpData.withColumn('geo{0}0'.format(6), get_geo6_udf('lat0', 'long0'))\n",
    "x = dropNullGeoJoin(xData.join(jData, on=[xData.geo60 == jData.Geo], how=\"left\"), \"Geo\").drop('geo{0}0'.format(6))\n",
    "geo6 = [y[0] for y in x.select('Geo').collect()]\n",
    "print(len(set(geo5).intersection(set(geos))))\n",
    "print(len(set(geo6).intersection(set(geos))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+-------------+---------------+-------------+-------------+---------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----------+-------------------+----------------+---------------+----------------+---------------+---------------+-----------+------------+-----------+------------+---+\n",
      "|Policy_Identifier|ADW_Enroll_ID|ADW_Member_ID|ADW_Operator_ID|CCT_Enroll_ID|CCT_Member_ID|CCT_Operator_ID|         device|              tripId|               Start|                 End|   tripStartLocation|     tripEndLocation|Duration|total_miles|tripTerminateReason|tripRejectReason|mobileAppDevice|mobileAppVersion|mobileOsVersion|inR4HTimePeriod|       lat0|       long0|       lat1|       long1|CSA|\n",
      "+-----------------+-------------+-------------+---------------+-------------+-------------+---------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----------+-------------------+----------------+---------------+----------------+---------------+---------------+-----------+------------+-----------+------------+---+\n",
      "|    8896986909256|      1160356|      1132575|         314273|      1165112|      1137404|         314804|003JT0MFK3HWSJ2|20180707003JT0MFK...|2018-07-07T11:22:...|2018-07-07T11:33:...|43.65136826,-70.2...|43.65795827,-70.2...|     641|        1.2|                  3|                |    LM-X410(FG)|          12.2.0|          7.1.2|              N|43.65136826|-70.27314191|43.65795827|-70.27317619|438|\n",
      "|    8896986909256|      1160356|      1132575|         314273|      1165112|      1137404|         314804|003JT0MFK3HWSJ2|20180707003JT0MFK...|2018-07-07T12:33:...|2018-07-07T12:47:...|43.65660023,-70.2...|43.65823138,-70.2...|     823|        2.6|                  3|                |    LM-X410(FG)|          12.2.0|          7.1.2|              N|43.65660023|-70.27195778|43.65823138|-70.29691181|438|\n",
      "|    8896986909256|      1160356|      1132575|         314273|      1165112|      1137404|         314804|003JT0MFK3HWSJ2|20180711003JT0MFK...|2018-07-11T13:02:...|2018-07-11T13:18:...|43.65818353,-70.2...|43.66406018,-70.3...|     975|        4.8|                  3|                |    LM-X410(FG)|          12.2.0|          7.1.2|              N|43.65818353|-70.29288821|43.66406018| -70.3681935|438|\n",
      "+-----------------+-------------+-------------+---------------+-------------+-------------+---------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------+-----------+-------------------+----------------+---------------+----------------+---------------+---------------+-----------+------------+-----------+------------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1\n",
      "6698\n",
      "726\n",
      "35\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def dropNullGeoJoin(spdf, colname):\n",
    "    return spdf.na.drop(subset=[colname])\n",
    "\n",
    "#tmpData.show(10)\n",
    "jData = spGeoCensusCSAMap\n",
    "geoData = []\n",
    "xData = tmpData.withColumn('geo{0}0'.format(3), get_geo3_udf('lat0', 'long0'))\n",
    "geoData.append(dropNullGeoJoin(xData.join(jData, on=[xData.geo30 == jData.Geo], how=\"left\"), \"Geo\").drop(\"Geo\").drop('geo{0}0'.format(3)))\n",
    "xData = tmpData.withColumn('geo{0}0'.format(4), get_geo4_udf('lat0', 'long0'))\n",
    "geoData.append(dropNullGeoJoin(xData.join(jData, on=[xData.geo40 == jData.Geo], how=\"left\"), \"Geo\").drop(\"Geo\").drop('geo{0}0'.format(4)))\n",
    "xData = tmpData.withColumn('geo{0}0'.format(5), get_geo5_udf('lat0', 'long0'))\n",
    "geoData.append(dropNullGeoJoin(xData.join(jData, on=[xData.geo50 == jData.Geo], how=\"left\"), \"Geo\").drop(\"Geo\").drop('geo{0}0'.format(5)))\n",
    "xData = tmpData.withColumn('geo{0}0'.format(6), get_geo6_udf('lat0', 'long0'))\n",
    "geoData.append(dropNullGeoJoin(xData.join(jData, on=[xData.geo60 == jData.Geo], how=\"left\"), \"Geo\").drop(\"Geo\").drop('geo{0}0'.format(6)))\n",
    "xData = tmpData.withColumn('geo{0}0'.format(7), get_geo7_udf('lat0', 'long0'))\n",
    "geoData.append(dropNullGeoJoin(xData.join(jData, on=[xData.geo70 == jData.Geo], how=\"left\"), \"Geo\").drop(\"Geo\").drop('geo{0}0'.format(7)))\n",
    "xData = tmpData.withColumn('geo{0}0'.format(8), get_geo8_udf('lat0', 'long0'))\n",
    "geoData.append(dropNullGeoJoin(xData.join(jData, on=[xData.geo80 == jData.Geo], how=\"left\"), \"Geo\").drop(\"Geo\").drop('geo{0}0'.format(8)))\n",
    "\n",
    "print(\"============================================================\")    \n",
    "for j in geoData:\n",
    "    print(j.count())\n",
    "#print(geoData[0].show(3))\n",
    "if False:\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "\n",
    "    cols = geo5Data.columns[1:]\n",
    "    start, cmt = clock(\"Augmenting data with {0} information\".format(name))\n",
    "    \n",
    "    gpsData = gpsData.join(geo5Data, on=[gpsData.geo50 == geo5Data.Geo5], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGeoData(suffix, prec=7):\n",
    "    from modelio import loadJoblib\n",
    "    from pickle import load\n",
    "    from timeUtils import clock, elapsed\n",
    "    startall,cmtall = clock(\"Loading Geo Data\")\n",
    "    geomapname = \"/home/tgadf/geomap-{0}-{1}.p\".format(prec, suffix)\n",
    "    start, cmt = clock(\"Loading {0}\".format(geomapname))\n",
    "    geomap     = load(open(geomapname, \"rb\"))\n",
    "    elapsed(start, cmt)\n",
    "    return geomap\n",
    "\n",
    "def castInt(coldata):\n",
    "    import pandas as pd\n",
    "    coldata = pd.to_numeric(coldata, downcast='signed', errors='coerce')\n",
    "    coldata = coldata.fillna(0)\n",
    "    return coldata\n",
    "\n",
    "def createGeoDataFrame(geomap, prec=7):\n",
    "    from timeUtils import clock, elapsed\n",
    "    from pandas import DataFrame\n",
    "    start, cmt = clock(\"Creating Pandas Data Frame\")\n",
    "    geodf = DataFrame(geomap).T\n",
    "    geodf.reset_index(inplace=True)\n",
    "    cols = list(geodf.columns)\n",
    "    cols[0] = \"Geo\" #{0}\".format(prec)\n",
    "    geodf.columns = cols\n",
    "\n",
    "    for col in geodf.columns:\n",
    "        geodf[col] = geodf[col].astype(str)\n",
    "        if col.startswith(\"Geo\") is False:\n",
    "            geodf[col] = castInt(geodf[col])\n",
    "        \n",
    "    elapsed(start, cmt)\n",
    "    return geodf\n",
    "\n",
    "def createGeohashPandasDataFrame(suffix, prec=7):\n",
    "    geomap = loadGeoData(suffix, prec)\n",
    "    geodf  = createGeoDataFrame(geomap)\n",
    "    return geodf\n",
    "\n",
    "def createGeohashSparkDataFrame(suffix, prec, scsql, inputDBName):    \n",
    "    from timeUtils import clock, elapsed\n",
    "    startall,cmtall = clock(\"Creating Spark DataFrame from {0}\".format(suffix))\n",
    "    pddf = createGeohashPandasDataFrame(suffix, prec)\n",
    "    \n",
    "    start, cmt = clock(\"Creating Spark DataFrame\")\n",
    "    spdf   = scsql.createDataFrame(pddf)\n",
    "    elapsed(start, cmt)\n",
    "\n",
    "    start, cmt = clock(\"Saving spark dataframe to HIVE\")\n",
    "    saveSparkData(spdf, inputDBName, \"Geo{0}Map\".format(suffix))\n",
    "    elapsed(startall, cmtall)\n",
    "    \n",
    "def getGeohashSparkDataFrame(suffix, prec, hc, scsql, inputDBName, force=False):\n",
    "    if force:\n",
    "        createGeohashSparkDataFrame(suffix, prec, scsql, inputDBName)\n",
    "        spdf = None\n",
    "    else:\n",
    "        try:\n",
    "            spdf = getHiveData(hc, inputDBName, \"Geo{0}Map\".format(suffix))\n",
    "            spdf.cache()\n",
    "            print(\"Total Rows = {0}\".format(spdf.count()))\n",
    "            print(\"   Columns = {0}\".format(spdf.columns))\n",
    "        except:\n",
    "            createGeohashSparkDataFrame(suffix, prec, scsql, inputDBName)\n",
    "            spdf = None\n",
    "    return spdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geohash Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinGeo5Data(gpsData, geo5Data, name, fillNA=None, debug=False):\n",
    "    if not all([isSparkDataFrame(x) for x in [gpsData, geo5Data]]):\n",
    "        print(\"Cannot join GPS and {0} data\".format(name))\n",
    "        return None\n",
    "    \n",
    "    ########################################################################################################################\n",
    "    ## Census\n",
    "    ########################################################################################################################\n",
    "    print(\"\\n================ Joining GPS and {0} Data ================\\n\".format(name))\n",
    "\n",
    "    cenprec=5\n",
    "    gpsData = gpsData.withColumn('geo{0}0'.format(cenprec), get_geo5_udf('lat0', 'long0')).withColumn('geo{0}1'.format(cenprec), get_geo5_udf('lat1', 'long1'))\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "\n",
    "    cols = geo5Data.columns[1:]\n",
    "    start, cmt = clock(\"Augmenting data with {0} information\".format(name))\n",
    "    \n",
    "    gpsData = gpsData.join(geo5Data, on=[gpsData.geo50 == geo5Data.Geo5], how=\"left\")\n",
    "    for col in cols:\n",
    "        gpsData = gpsData.withColumnRenamed(col, 'Geo0{0}ID'.format(col))\n",
    "    gpsData = gpsData.drop('Geo5').drop('geo50')\n",
    "    if fillNA is not None:\n",
    "        gpsData = gpsData.na.fill(fillNA)\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "\n",
    "    gpsData = gpsData.join(geo5Data, on=[gpsData.geo51 == geo5Data.Geo5], how=\"left\")\n",
    "    for col in cols:\n",
    "        gpsData = gpsData.withColumnRenamed(col, 'Geo1{0}ID'.format(col))\n",
    "    gpsData = gpsData.drop('Geo5').drop('geo51')\n",
    "    if fillNA is not None:\n",
    "        gpsData = gpsData.na.fill(fillNA)\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "    \n",
    "    elapsed(start, cmt)\n",
    "    \n",
    "    return gpsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joinGeo7Data(gpsData, geo7Data, name, fillNA=None, debug=False):\n",
    "    if not all([isSparkDataFrame(x) for x in [gpsData, geo7Data]]):\n",
    "        print(\"Cannot join GPS and {0} data\".format(name))\n",
    "        return None\n",
    "    \n",
    "    ########################################################################################################################\n",
    "    ## Census\n",
    "    ########################################################################################################################\n",
    "    print(\"\\n================ Joining GPS and {0} Data ================\\n\".format(name))\n",
    "\n",
    "    cenprec=7\n",
    "    gpsData = gpsData.withColumn('geo{0}0'.format(cenprec), get_geo7_udf('lat0', 'long0')).withColumn('geo{0}1'.format(cenprec), get_geo7_udf('lat1', 'long1'))\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "\n",
    "    cols = geo7Data.columns[1:]\n",
    "    start, cmt = clock(\"Augmenting data with {0} information\".format(name))\n",
    "    \n",
    "    gpsData = gpsData.join(geo7Data, on=[gpsData.geo70 == geo7Data.Geo7], how=\"left\")\n",
    "    for col in cols:\n",
    "        gpsData = gpsData.withColumnRenamed(col, 'Geo0{0}ID'.format(col))\n",
    "    gpsData = gpsData.drop('Geo7').drop('geo70')\n",
    "    if fillNA is not None:\n",
    "        gpsData = gpsData.na.fill(fillNA)\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "\n",
    "    gpsData = gpsData.join(geo7Data, on=[gpsData.geo71 == geo7Data.Geo7], how=\"left\")\n",
    "    for col in cols:\n",
    "        gpsData = gpsData.withColumnRenamed(col, 'Geo1{0}ID'.format(col))\n",
    "    gpsData = gpsData.drop('Geo7').drop('geo71')\n",
    "    if fillNA is not None:\n",
    "        gpsData = gpsData.na.fill(fillNA)\n",
    "    if debug:\n",
    "        gpsData.show(3)\n",
    "    \n",
    "    elapsed(start, cmt)\n",
    "    \n",
    "    return gpsData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Census\n",
    "gpsData = joinGeo5Data(gpsData, spGeoCensusMap, fillNA=\"NA\", name=\"Census\")\n",
    "\n",
    "## HERE\n",
    "gpsData = joinGeo7Data(gpsData, spGeoHEREMap, fillNA=0, name=\"HERE\")\n",
    "\n",
    "## OSM Traffic\n",
    "gpsData = joinGeo7Data(gpsData, spOSMTrafficMap, fillNA=0, name=\"TrafficOSM\")\n",
    "\n",
    "## OSM Transit\n",
    "gpsData = joinGeo7Data(gpsData, spOSMTransportMap, fillNA=0, name=\"TransportOSM\")\n",
    "\n",
    "## OSM Powf\n",
    "gpsData = joinGeo7Data(gpsData, spOSMPofwMap, fillNA=0, name=\"PowfOSM\")\n",
    "\n",
    "## OSM POI\n",
    "gpsData = joinGeo7Data(gpsData, spOSMPOIMap, fillNA=0, name=\"PoisOSM\")\n",
    "\n",
    "## Interstate\n",
    "gpsData = joinGeo7Data(gpsData, spGeoInterstateMap, fillNA=0, name=\"Interstate\")\n",
    "\n",
    "## USRte\n",
    "gpsData = joinGeo7Data(gpsData, spGeoUSRteMap, fillNA=0, name=\"USRte\")\n",
    "\n",
    "## StateRte\n",
    "gpsData = joinGeo7Data(gpsData, spGeoStateRteMap, fillNA=0, name=\"StateRte\")\n",
    "\n",
    "## Highway\n",
    "gpsData = joinGeo7Data(gpsData, spGeoHighwayMap, fillNA=0, name=\"Highway\")\n",
    "\n",
    "## MajorRd\n",
    "gpsData = joinGeo7Data(gpsData, spGeoMajorRdMap, fillNA=0, name=\"MajorRd\")\n",
    "\n",
    "## POI\n",
    "gpsData = joinGeo7Data(gpsData, spGeoPOIMap, fillNA=0, name=\"POI\")\n",
    "\n",
    "## Terminals\n",
    "gpsData = joinGeo7Data(gpsData, spGeoTerminalsMap, fillNA=0, name=\"Terminals\")\n",
    "\n",
    "\n",
    "if False:\n",
    "    ## OSM Buildings\n",
    "    gpsData = joinGeo7Data(gpsData, spGeoHEREMap, fillNA=0, name=\"buildingsOSM\")\n",
    "\n",
    "    ## Venue\n",
    "    gpsData = joinGeo7Data(gpsData, spGeoVenueMap, fillNA=0, name=\"Venue\")\n",
    "\n",
    "    ## College\n",
    "    gpsData = joinGeo7Data(gpsData, spGeoCollegeMap, fillNA=0, name=\"College\")\n",
    "\n",
    "    ## School\n",
    "    gpsData = joinGeo7Data(gpsData, spGeoSchoolMap, fillNA=0, name=\"School\")\n",
    "\n",
    "    ## Auto\n",
    "    gpsData = joinGeo7Data(gpsData, spGeoAutoMap, fillNA=0, name=\"Auto\")\n",
    "\n",
    "    ## Rail\n",
    "    gpsData = joinGeo7Data(gpsData, spGeoRailMap, fillNA=0, name=\"Rail\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpsData2017 = gpsData\n",
    "gpsData2017.cache()\n",
    "print(\"Total Rows = {0}\".format(gpsData2017.count()))    \n",
    "print(\"   Columns = {0}\".format(gpsData2017.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpsData2018 = gpsData\n",
    "gpsData2018.cache()\n",
    "print(\"Total Rows = {0}\".format(gpsData2018.count()))    \n",
    "print(\"   Columns = {0}\".format(gpsData2018.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "gpsData = unionAll(gpsData2017, gpsData2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveSparkData(gpsData, inputDBName, \"r4happend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSPdf,testPDdf = saveDeviceTrips(gpsData, None, 'r4hNot', savePandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True and isLocal is False:\n",
    "    tmp = gpsData.filter(\"Geo0TractID is NULL\")\n",
    "    tmp = tmp.select('lat0', 'long0', 'geo0')\n",
    "    print(\"Saving NoTractData!\")\n",
    "    tmp.toPandas().to_csv('notract.csv')\n",
    "\n",
    "if False and isLocal is False:    \n",
    "    counts = gpsData.groupBy('device').count().toPandas()\n",
    "    counts.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "    touse=counts['device'].tolist()[:200]\n",
    "    print(\", \".join(touse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset the data if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if isLocal is False:\n",
    "    def distHash(gcode1, gcode2):\n",
    "        \"\"\"\n",
    "        distHash(gcode1, gcode2)\n",
    "\n",
    "        inputs: gcode1 (geohash), gcode2 (geohash)\n",
    "        outputs: distance (km)\n",
    "        \"\"\"\n",
    "        pnt1 = geohash.decode_exactly(gcode1)[:2]\n",
    "        pnt2 = geohash.decode_exactly(gcode2)[:2]\n",
    "        dist = haversine(pnt1, pnt2)\n",
    "        return dist\n",
    "\n",
    "    def getCityData(cityname):\n",
    "        poi = loadJoblib('poi-7.p')\n",
    "        cities={}\n",
    "        if False:\n",
    "            cities['naperville'] = {\"lat\": 41.750839, \"long\": -88.153535, \"radius\": 0.05}\n",
    "            cities['knoxville']  = {'lat': 35.964668, 'long': -83.926453, 'radius': 0.15}\n",
    "            cities['wrigley']    = {'lat': 41.948437, 'long': -87.655334, 'radius': 0.03} # Wrigley Field\n",
    "            cities['alexandria'] = {'lat': 38.80472,  'long': -77.04722, 'radius': 0.075}  # Alexandria, VA\n",
    "            cities['dallas']     = {'long': -97.04044, 'lat': 32.897480, 'radius': 0.15}\n",
    "            cities['urbana']     = {'long': -88.24338, 'lat': 40.116421, 'radius': 0.15}\n",
    "            cities['portjeff']   = {'long': -73.06927, 'lat': 40.946487, 'radius': 0.15}\n",
    "        cities['r4h']        = {'lat': 38.88184,  'long': -77.10806, 'radius': 0.1} # r4h\n",
    "        for city,citydata in cities.items():\n",
    "            if cityname != city:\n",
    "                continue\n",
    "            print(\"--> {0}\".format(city))\n",
    "            geocity = geohash.encode(citydata['lat'], citydata['long'], precision=7)\n",
    "            poicity = {}\n",
    "            for k,v in poi.items():\n",
    "                poicity[k] = set()\n",
    "                for geo in v:\n",
    "                    try:\n",
    "                        dist = distHash(geocity, geo)\n",
    "                    except:\n",
    "                        continue\n",
    "                    if dist < citydata['radius']*111:\n",
    "                        poicity[k].add(geo)\n",
    "                print(\"  Keeping {0} out of {1}\".format(len(poicity[k]), len(v)))\n",
    "            print(\"--> {0}\".format(city))\n",
    "            saveJoblib(poicity, 'poi-{0}.p'.format(city))\n",
    "\n",
    "\n",
    "            lat0 = citydata['lat']\n",
    "            long0 = citydata['long']\n",
    "            dl = citydata['radius']\n",
    "            test = gpsData\n",
    "            test = test.filter('lat0 > {0}'.format(lat0-dl))\n",
    "            test = test.filter('lat0 < {0}'.format(lat0+dl))\n",
    "            test = test.filter('long0 < {0}'.format(long0+dl))\n",
    "            test = test.filter('long0 > {0}'.format(long0-dl))\n",
    "            test = test.filter('lat1 > {0}'.format(lat0-dl))\n",
    "            test = test.filter('lat1 < {0}'.format(lat0+dl))\n",
    "            test = test.filter('long1 < {0}'.format(long0+dl))\n",
    "            test = test.filter('long1 > {0}'.format(long0-dl))\n",
    "            print(\"There are {0} rows in this data\".format(test.count()))\n",
    "\n",
    "            counts = test.groupBy('device').count().toPandas()\n",
    "            counts.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "            touse=counts['device'].tolist()[:200]\n",
    "            print(\", \".join(touse))\n",
    "            \n",
    "    getCityData('r4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isLocal is False:\n",
    "    try:\n",
    "        1/0\n",
    "        testSPdf = getHiveData(hc, inputDBName, \"testNetworkTable\")\n",
    "        testSPdf.cache()\n",
    "    except:\n",
    "        savePandas=False\n",
    "        saveSPdf=True\n",
    "        touse=['CHR1VHPYJOCHQPE', 'IS3XXESAIT8L7JQ', '025Y89CNOVL7M6T', '1W0L9CUYZKLC2EH', 'KV1LY17CFLOJEPO', 'THLIUIW5MLET3TD', 'EVO43H0OZ1891SG', '16ZIWIW0ZKCUVP0', 'EPO7MNGEWEB5A1O', 'JQAAQNT14S0A1FG', 'RS9KSH4GAAKW0HU', 'RBHRZPW4FAAAJ8W', 'QZE1TJP8BN4KIV4', 'VVIW09JJLL1O3DF', 'HOAGN03E70MTE1M', '1A0CS88DINJBIN0', '5UWVYLOUNI32SSK', 'C7U0B2N53EJIMUF', 'OHD6Z42Q7ECHWTM', 'GIRPNJ941S0DJZ8', 'R8ZCEHQCSI65PNU', 'BEVTKC4BDQQ14OP', 'LQHRNV1BZI2L84U', '02SZVM3K17UJPM5', 'L23BGHTNJ1CXV1B', 'IYGPJ1PUMU56DL4', 'CPGHR562L6GPWIE', 'HW5FEU8YHA62JWS', '26WGI6ZS4C2C8TZ', 'Y81QXYAVCS6N6YG', '0DRBJ7Y93D8IUZC', 'CCS3PJZFU79D85B', '4ETQIFWVWS7U9HT', 'QIO27XPVVLB4X4P', 'BT16QXY82T8WS18', 'EMAQM1SCIKUYYO4', '174MJHGHYBB4AWO', 'AAYIHQ9XDW8LY1P', 'BX6MMF5T9EGXBTQ', 'VGSSQG41NB6MMP5', '4CVWEU85854SGGC', 'ZL2XSDZOG2WQCNS', 'JLRI1S0UXHP2W9L', 'SDL4FYC5SUPMGKM', '1P1PROI93U5VBK8', 'F0JMBVEMTI0YD76', 'AOVS27L7GLDB1MB', 'OT82MGHMQVHVTAA', '27LL22GI5QX6G2I', 'HC15603LOJ0GVCR', '21D8J8P4FHXWS95', 'B6EKFX3A6GXSRU2', 'KN1AA4G1QSGAG0S', '6U51EKFNJMRR89T', 'Y2FY4ZDDPXROHRL', 'E7PY6WY05B3EA3L', 'HU6CEAML486T9OT', '7624ZGZ4JL5T2JU', 'SH5V63L92H644QG', '496B5OZ3JRW57L8', 'X4C5VC6ACGEOMYR', 'SGAXETSMRD2PC9L', 'PDWX0IVKSKQJ18N', 'IQ8X9Q8VED2CT2H', '064USL6GN4JXZW5', 'IBI92M9BVN6KUFU', 'AFN57RW69W1POWM', 'GZ95CHZ10ZUL21Q', 'PCBJ0ZUBX2LKUDC', 'VJAX94RCMO1QZGM', '9Z9HTI8BQYEPM5L', '3QWKN1GUBPXV1YG', 'U634DFGQ46CVZU1', '64TSVKTBGE6MUC4', 'LL2DKPE8VZJHOIQ', '3UGZH50IXU5M9GC', 'RPBOYKOUCKBEKWZ', '0W3WBE75JUPYBGB', '2K3KOAIGZ8H7E9F', 'EA0H49UFAHFIF1O', '2PO0ILNAH0Y6H5H', 'L9XWIWAS7TXZ5EI', '0DECP5BGAZ4D7OK', 'OM891TXIKHPFCEM', 'WT9PSHKQQNTONDQ', '34DW4A69OHV9VEA', 'PTE6489IRYPJZK9', 'G5736JP5MDWAO7R', 'HWPR7MYXNIOA1YM', '10EIURE3JVZWQOB', '7WU2IRYX9K9DFAK', 'V8B440NXJ9V36E1', '321T0ODW2B1X4NS', 'W0CLLCZP0PR9GT8', '76B42YPIOSB5PP5', 'GTPWG21BY6DNJ1A', 'M5IP1YYLH2T4MTV', 'LX80T9LCBJJP1A8', '8QPXFB0NR9XSL1R', 'QQHYEH1QSZ9BJ4C', '16WHBD18ANFXSC7', 'VBPQ1RZOHXQPU33', '5IRLIJ835DZQ7C9', 'S6ODHD1JNPP3LEF', 'VCKG70S09GFC18Y', 'KG9RCO0YRZNU229', 'X095KVI0YHCMXW8', 'SCGTEG1ZHDJCWUV', '8DHZIIXNRE2WDMO', 'B7YAFVR8T8UCO7T', 'U9FU6AB9TDVVASA', '6QE1FZ47BX9V8XV', 'H49QLSOALH391XB', 'EWULP0RUHTTTGSY', 'BWIXVB2VPFPL944', 'VND6SY2FADSI1XJ', 'N6KDDLYQDWEY03R', 'MO62RDISA9ZV2XE', 'A8PHV89STDXGJH0', '6QCF7KG3JDOZBOV', '75B3PTOBC1MSVW3', '4SREWJ404ADPL7G', 'FFE2PH8NGP9MGFR', 'RV47BQ3E7UF9ECI', 'SAPBLSYZE6Y6VQB', 'JESTIZQ3RO8AQDN', 'TFSWNYZPCQE66ZX', 'MNYI2VKFDE16FQZ', 'HJKJO1K2XKJOUP7', 'AAU4UFKKIBRAMGC', 'GDSBM6GB26J3N57', 'KGFLPA85K7IH92O', 'GKIIUA9CDSXPXYJ', 'TACCU73WROPWJZY', '03462RPFLK6E1SC', '4YVUNWIOVX59NBE', 'H6ZC3U4RMI811UX', 'AB3XSJDBYLWE7HF', 'R32N8RG51ZFBRKE', '8YOANILU9ZC92XN', 'BNN8BA3PKJ7R586', 'HNULR1ZEIF4FJCO', 'W4WQQYRHFSQPWWF', 'BRGN43WQOFJBX6T', '77C16XBOK1E1TDX', 'ZIA83JXA3WMZXY5', '3L3T2M4XV48L9YA', 'O7C8CK9XK9TRHK5', '1TZW5S459KEW0WL', '5SY6WCZNXSSDQYR', 'IE4O0EYAN1PIRI4', '7F730A7999IYVMI', 'SNH21FUYX7186QX', 'RO4SHGLP17FB0D6', 'MN9CR68XFO090C9', 'CV2RUI0PIK75M0N', 'S1M2FU7C3SMG9XM', 'AU6RTXA62SCS2SH', 'LZ4DP1NHJJG20AG', 'YWD7K7RNTHQXX92', '0Z78Q2TFZEB7T39', 'VO5BN4UUTU9CPP5', 'LBZUVTNJSYX7FC5', '0PQXGJLTD89UXTD', 'RXJ3XFFEDJRIK5C', 'N3R73MPTBHXALLF', 'HG5DGOAQBDWBUF9', 'QE54JE02BKJE6HI', 'QQV9CD0C2VUVZK7', 'SP1X0K7ASWKCBSM', 'XU0SF1AXLKK88RP', 'FH039Y5D0O0OHTA', '5FB6YYUH3LLPLW5', 'G95T3O2LXMA53Z9', '922OAUBOIB997SA', 'DJ2S7HP4RSRH2H8', '4HQ1E7F6JWY5DE1', 'D3WQPZVPHXTAXGC', 'XLCTF0OYKMDY9X0', 'VZ7TZKBHVQ0LU6A', '71JUGFNNYU0564P', 'JUHDWSKJDGCBZOS', 'Y7E9LQ2Y6OTUV20', 'D2444UOK8WLK30Y', 'UFJ8MFSUQBUHIV1', 'HJONJKLEJ8DSN8B', 'A5AEA60GEJ2W3P2', 'DK82UHRLKDPMAI9', 'UT1LDDN38LMI4UR', 'DUUSJT0P6WEMKU4', 'SO5WCZ95O14RSQ3', 'V1VV83TBXDBKTAZ', '22J2PERM3KVJGL3', 'IM9MXK1U0TL8PD2', '5C79WQ9EJAHA1Y9', 'W632ATN1WKHG1C4', 'V9BHATWO8G2PXC6', 'Y6NBP0Z7TFIDSNG', 'TI391INCV3QJQUT', 'U2SPM7E9HWNZQ6J']\n",
    "        touse=None\n",
    "        testSPdf,testPDdf = saveDeviceTrips(gpsData, touse, 'testNetworkTable', savePandas=savePandas)\n",
    "\n",
    "        if saveSPdf:\n",
    "            start, cmt = clock(\"Saving spark dataframe to HIVE\")\n",
    "            saveSparkData(testSPdf, inputDBName, \"testNetworkTable\")\n",
    "            elapsed(start, cmt)\n",
    "else:\n",
    "    print(\"Loading Device Trips in Local Mode\")\n",
    "    #highgeoppd = loadDeviceTrips(\"highgeo\")\n",
    "    testPDdf = loadDeviceTrips('r4h')\n",
    "    touse = list(testPDdf['device'].unique())\n",
    "    print(\"There are {0} devices in this table\".format(len(touse)))\n",
    "    #testPDdf = testPDdf[testPDdf['device'] == '353162070041042']\n",
    "    print(\"There are {0} devices in this table\".format(testPDdf['device'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSPdf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if isLocal is True:\n",
    "    start = clock(\"Running manually\")\n",
    "    %load_ext autoreload\n",
    "    %autoreload\n",
    "    from network import extract_features\n",
    "    from geoClustering import geoClusters\n",
    "    from networkCategories import categories\n",
    "    from timeUtils import getDateTime, printDateTime, clock, elapsed, getDateTime\n",
    "    pddata = {}\n",
    "    i=0\n",
    "    for device, geodata in testPDdf.groupby('device'):\n",
    "        start = clock(device, showTime=False)\n",
    "        i += 1\n",
    "        print(device,i,testPDdf['device'].nunique(),geodata.shape[0])\n",
    "        #if i < 25:\n",
    "        #    continue\n",
    "        retval = extract_features(geodata, debug=False, numFeats=1071)\n",
    "        pddata[device] = retval\n",
    "        print(\"  --> {0}\".format(retval.shape))\n",
    "        elapsed(start, device, showTime=False)\n",
    "        break\n",
    "    elapsed(start, \"Done running manually\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isLocal is True:\n",
    "    retval.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if isLocal is False:\n",
    "    start, cmt = clock(\"Computing Graph Features\")\n",
    "    deviceNetworkData = (testSPdf\n",
    "                          .repartition(200, 'device')           # turn up parallelism to ensure we're not overloading an executor\n",
    "                          .rdd                                  # going to be using the rdd api here\n",
    "                          .map(lambda row: (row.device, row))   # key by the dev_imei for now, but it might be better to also key by vin\n",
    "                          .groupByKey()                         # rdd of (dev_imei, [row1, row2...])\n",
    "                          .mapValues(rows_to_pandas_df)         # rdd of (dev_imei, df) pairs where df is all the data from that dev_imei\n",
    "                          .mapValues(extract_features)          # rdd of (dev_imei, df) pairs where df that dev_imei's extracted features\n",
    "                          .map(lambda tpl: tpl[1])              # drop the key, now just rdd of df's\n",
    "                          .flatMap(pandas_df_to_rows)           # now rdd of Row's where each Row represents a device day\n",
    "                          .toDF()                               # convert back to dataframe\n",
    "                             )\n",
    "    elapsed(start, cmt)\n",
    "    start, cmt = clock(\"Counting rows\")\n",
    "    print(\"There are {0} rows in deviceNetworkData\".format(deviceNetworkData.count()))\n",
    "    elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnData = deviceNetworkData.toPandas()\n",
    "#dnData\n",
    "\n",
    "from modelio import saveJoblib\n",
    "saveJoblib(dnData, filename=\"/home/tgadf/astro_research_data/futuremiles/gpsData/portjeffNetworkFeatures.p\", compress=True)\n",
    "elapsed(start, \"Saved Driving Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Full Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isLocal is False:\n",
    "    start, cmt = clock(\"Computing Graph Features\")\n",
    "    deviceNetworkData = (gpsData\n",
    "                          .repartition(50000, 'device')         # turn up parallelism to ensure we're not overloading an executor\n",
    "                          .rdd                                  # going to be using the rdd api here\n",
    "                          .map(lambda row: (row.device, row))   # key by the dev_imei for now, but it might be better to also key by vin\n",
    "                          .groupByKey()                         # rdd of (dev_imei, [row1, row2...])\n",
    "                          .mapValues(rows_to_pandas_df)         # rdd of (dev_imei, df) pairs where df is all the data from that dev_imei\n",
    "                          .mapValues(extract_features)          # rdd of (dev_imei, df) pairs where df that dev_imei's extracted features\n",
    "                          .map(lambda tpl: tpl[1])              # drop the key, now just rdd of df's\n",
    "                          .flatMap(pandas_df_to_rows)           # now rdd of Row's where each Row represents a device day\n",
    "                          .toDF()                               # convert back to dataframe\n",
    "                             )\n",
    "    elapsed(start, cmt)\n",
    "\n",
    "\n",
    "    start, cmt = clock(\"Counting and caching results\")\n",
    "    deviceNetworkData.cache()\n",
    "    print(\"There are {0} devices with network parameters.\".format(deviceNetworkData.count()))\n",
    "    elapsed(start, cmt)\n",
    "\n",
    "\n",
    "    start, cmt = clock(\"Saving spark dataframe to HIVE\")\n",
    "    saveSparkData(deviceNetworkData, inputDBName, networkTableName)\n",
    "    elapsed(start, cmt)\n",
    "\n",
    "\n",
    "    start, cmt = clock(\"Creating Pandas DF\")\n",
    "    deviceNetworkPandasData = deviceNetworkData.toPandas()\n",
    "    elapsed(start, cmt)\n",
    "\n",
    "    from modelio import saveJoblib\n",
    "    start, cmt = clock(\"Saving Pandas DF\")\n",
    "    saveJoblib(deviceNetworkPandasData, filename=\"/home/tgadf/astro_research_data/futuremiles/gpsData/drivingNetworkFeatures.p\", compress=True)\n",
    "    elapsed(start, cmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = clock(comment=\"Getting GSP Trip Endpoint Data\")\n",
    "deviceNetworkData = getHiveData(hc, inputDBName, networkTableName)\n",
    "deviceNetworkData.cache()\n",
    "elapsed(start, comment=\"Got GPS Trip Endpoint Data\")\n",
    "start = clock(\"\")\n",
    "print(\"There are {0} devices with network parameters.\".format(deviceNetworkData.count()))\n",
    "elapsed(start, \"Done counting and caching results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=clock(\"Creating Pandas DF\")\n",
    "deviceNetworkPandasData = deviceNetworkData.toPandas()\n",
    "\n",
    "from modelio import saveJoblib\n",
    "saveJoblib(deviceNetworkPandasData, filename=\"/home/tgadf/astro_research_data/futuremiles/gpsData/drivingNetworkFeatures.p\", compress=True)\n",
    "elapsed(start, \"Saved Driving Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = highgeoppd\n",
    "debug=True\n",
    "import pandas as pd\n",
    "import pygeohash\n",
    "from numpy import amax\n",
    "from geoClustering import geoClusters\n",
    "\n",
    "\n",
    "## Make sure everything is sorted by time\n",
    "if debug is True:\n",
    "    start = clock(\"Sorting data by time\")\n",
    "df.sort_values(by=\"Start\", ascending=True, inplace=True)\n",
    "if debug is True:\n",
    "    elapsed(start, \"Done sorting data by time\")\n",
    "\n",
    "devices = list(df['device'].unique())\n",
    "current_device = str(devices[0])\n",
    "if len(devices) > 1:\n",
    "    raise ValueError(\"There are [{0}] multiple devices\".format(devices))\n",
    "\n",
    "if debug:\n",
    "    start = clock(\"Loading points of interest file\")\n",
    "from modelio import loadJoblib\n",
    "poi = loadJoblib(\"poi-7.p\")\n",
    "if debug:\n",
    "    elapsed(start, \"Finished loading points of interest file\")\n",
    "\n",
    "points = df[[\"lat0\", \"long0\"]]\n",
    "points.columns = [\"lat\", \"long\"]\n",
    "pnts = df[[\"lat1\", \"long1\"]]\n",
    "pnts.columns = [\"lat\", \"long\"]    \n",
    "points = points.append(pnts)\n",
    "\n",
    "if debug:\n",
    "    start = clock(\"Finding clusters\")\n",
    "gc = geoClusters(points=points)\n",
    "gc.setPOI(poi)\n",
    "gc.findClusters(seedMin=3, debug=False)\n",
    "gc.findPOIClusters()\n",
    "#gc.printClusters()\n",
    "if debug:\n",
    "    elapsed(start, \"Finished finding clusters (0)\".format(gc.getNClusters()))\n",
    "\n",
    "\n",
    "if debug:\n",
    "    start = clock(\"Finding nearest cluster for start of trip\")\n",
    "geoResults = df[['lat0', 'long0']].apply(gc.getNearestClusters, axis=1).values\n",
    "df[\"geo0\"] = [x[0] for x in geoResults]\n",
    "if debug:\n",
    "    elapsed(start, \"Done finding nearest cluster for start of trip\")\n",
    "    start = clock(\"Finding nearest cluster for end of trip\")\n",
    "geoResults = df[['lat1', 'long1']].apply(gc.getNearestClusters, axis=1).values\n",
    "df[\"geo1\"] = [x[0] for x in geoResults]    \n",
    "if debug:\n",
    "    elapsed(start, \"Done finding nearest cluster for end of trip\")\n",
    "\n",
    "from networkTrips import getTripsFromPandas\n",
    "trips = getTripsFromPandas(df, gc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis With Full Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = clock(comment=\"Getting GSP Trip Endpoint Data\")\n",
    "deviceNetworkData = getHiveData(hc, inputDBName, networkTableName)\n",
    "elapsed(start, comment=\"Got GPS Trip Endpoint Data\")\n",
    "start = clock(\"Counting Rows\")\n",
    "deviceNetworkData.cache()\n",
    "print(\"There are {0} rows in this data frame\".format(deviceNetworkData.count()))\n",
    "elapsed(start, \"Done Counting Rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNetworkPandasData = deviceNetworkData.toPandas()\n",
    "from modelio import saveJoblib, loadJoblib\n",
    "saveJoblib(deviceNetworkPandasData, filename=\"/home/tgadf/astro_research_data/futuremiles/gpsData/drivingNetworkFeatures.p\", compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviceNetworkPandasData[deviceNetworkPandasData['device'] == '\n",
    "    highgeodata = gpsData.filter('device in '+use_devs)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveDeviceTrips(deviceNetworkData, touse=None, name=\"newNetworkFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelio import saveJoblib, loadJoblib\n",
    "pdData = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/newNetworkFeatures.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdData[pdData['device'] == '352252060403962']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isLocal:\n",
    "    from modelio import loadJoblib\n",
    "    from pickle import load\n",
    "    from timeUtils import clock, elapsed\n",
    "    from pandas import DataFrame\n",
    "    geoStateData5  = load(open(\"/home/tgadf/geoState-5.p\", \"rb\"))\n",
    "    geoCountyData5 = load(open(\"/home/tgadf/geoCounty-5.p\", \"rb\"))\n",
    "    geoTractData5  = load(open(\"/home/tgadf/geoTract-5.p\", \"rb\"))\n",
    "\n",
    "    geoStateData4  = load(open(\"/home/tgadf/geoState-4.p\", \"rb\"))\n",
    "    geoCountyData4 = load(open(\"/home/tgadf/geoCounty-4.p\", \"rb\"))\n",
    "    geoTractData4  = load(open(\"/home/tgadf/geoTract-4.p\", \"rb\"))\n",
    "\n",
    "    start = clock(\"Creating Geo4,5 Pandas DataFrames\")\n",
    "    df1 = DataFrame(list(zip(geoStateData4.keys(), geoStateData4.values())), columns=['Geo4', 'StateID'])\n",
    "    df2 = DataFrame(list(zip(geoCountyData4.keys(), geoCountyData4.values())), columns=['Geo4', 'CountyID'])\n",
    "    df3 = DataFrame(list(zip(geoTractData4.keys(), [x[:11] for x in geoTractData4.values()])), columns=['Geo4', 'TractID'])\n",
    "    df4 = DataFrame(list(zip(geoTractData4.keys(), geoTractData4.values())), columns=['Geo4', 'BlkGrpID'])\n",
    "    dfGeo4 = df1.merge(df2, on='Geo4', how='left').merge(df3, on=\"Geo4\", how='left').merge(df4, on=\"Geo4\", how='left')\n",
    "\n",
    "    df1 = DataFrame(list(zip(geoStateData5.keys(), geoStateData5.values())), columns=['Geo5', 'StateID'])\n",
    "    df2 = DataFrame(list(zip(geoCountyData5.keys(), geoCountyData5.values())), columns=['Geo5', 'CountyID'])\n",
    "    df3 = DataFrame(list(zip(geoTractData5.keys(), [x[:11] for x in geoTractData5.values()])), columns=['Geo5', 'TractID'])\n",
    "    df4 = DataFrame(list(zip(geoTractData5.keys(), geoTractData5.values())), columns=['Geo5', 'BlkGrpID'])\n",
    "    dfGeo5 = df1.merge(df2, on='Geo5', how='left').merge(df3, on=\"Geo5\", how='left').merge(df4, on=\"Geo5\", how='left')\n",
    "    elapsed(start, \"Done with Geo Pandas DataFrame\")\n",
    "\n",
    "    start = clock(\"Creating Spark DataFrames\")\n",
    "    spdfGeo4   = scsql.createDataFrame(dfGeo4)\n",
    "    spdfGeo5   = scsql.createDataFrame(dfGeo5)\n",
    "    elapsed(start, \"Done with Spark DataFrames\")\n",
    "\n",
    "    if False:\n",
    "        start = clock(\"Loading all data\")\n",
    "        blockpopdata    = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_BlockGroup_Pop.p\")\n",
    "        tractpopdata    = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_Tract_Pop.p\")\n",
    "        tracthousedata  = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_Tract_House.p\")\n",
    "        tractareadata   = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_Tract_Area.p\")\n",
    "        urbanstatedata  = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_State.p\")\n",
    "        urbancountydata = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_County.p\")\n",
    "        urbanuapopdata  = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_County_Urban_PopDensity.p\")\n",
    "        urbanrapopdata  = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_County_Rural_PopDensity.p\")\n",
    "        urbanucpopdata  = loadJoblib(\"/home/tgadf/astro_research_data/futuremiles/gpsData/census/results/GeoID_County_Cluster_PopDensity.p\")\n",
    "        elapsed(start, \"Done loading data\")\n",
    "\n",
    "        start = clock(\"Creating Pandas DataFrame\")\n",
    "        df1 = DataFrame(list(zip(urbanstatedata.keys(), urbanstatedata.values())), columns=['CountyID', 'State'])\n",
    "        df2 = DataFrame(list(zip(urbancountydata.keys(), urbancountydata.values())), columns=['CountyID', 'County'])\n",
    "        df3 = DataFrame(list(zip(urbanuapopdata.keys(), urbanuapopdata.values())), columns=['CountyID', 'CountyUrbanAreaPopPct'])\n",
    "        df4 = DataFrame(list(zip(urbanrapopdata.keys(), urbanrapopdata.values())), columns=['CountyID', 'CountyRuralAreaPopPct'])\n",
    "        df5 = DataFrame(list(zip(urbanucpopdata.keys(), urbanucpopdata.values())), columns=['CountyID', 'CountyUrbanClusterPopPct'])\n",
    "        dfCounty = df1.merge(df2, on='CountyID', how='left').merge(df3, on=\"CountyID\", how='left').merge(df4, on=\"CountyID\", how='left').merge(df5, on=\"CountyID\", how='left')\n",
    "\n",
    "        df1 = DataFrame(list(zip(tractpopdata.keys(), tractpopdata.values())), columns=['TractID', 'TractPop'])\n",
    "        df2 = DataFrame(list(zip(tracthousedata.keys(), tracthousedata.values())), columns=['TractID', 'TractHouses'])\n",
    "        df3 = DataFrame(list(zip(tractareadata.keys(), tractareadata.values())), columns=['TractID', 'TractArea'])\n",
    "        dfTract = df1.merge(df2, on='TractID', how='left').merge(df3, on=\"TractID\", how='left')\n",
    "\n",
    "        dfBlkGrp = DataFrame(list(zip(blockpopdata.keys(), blockpopdata.values())), columns=['BlkGrpID', 'BlkGrpPop'])\n",
    "        elapsed(start, \"Done with Pandas DataFrames\")\n",
    "\n",
    "        start = clock(\"Creating Spark DataFrames\")\n",
    "        spdfCounty = scsql.createDataFrame(dfCounty)\n",
    "        spdfTract  = scsql.createDataFrame(dfTract)\n",
    "        spdfBlkGrp = scsql.createDataFrame(dfBlkGrp)\n",
    "        elapsed(start, \"Done with Spark DataFrames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = '1234567'\n",
    "x[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "if False:\n",
    "    from driverNetwork import driverNetwork\n",
    "    dn = driverNetwork(trips)\n",
    "    dn.createNetwork(debug=True)\n",
    "    dn.setVertexOrder(debug=True)\n",
    "    dn.setEdgeOrder(debug=True)\n",
    "    dn.setNetworkAttributes(debug=True)\n",
    "\n",
    "    from networkFeatures import networkFeatures\n",
    "    nf = networkFeatures(dn)\n",
    "    nf.setGlobalNetworkFeatures(debug=True)\n",
    "    nf.setArticulationStructure(debug=True)\n",
    "    #nf.setCliqueStructure(debug=True)\n",
    "    nf.setCommunityStructure(debug=True)\n",
    "    nf.setDyadCensus(debug=True)\n",
    "    nf.setEdgeCorrelations(debug=True)\n",
    "    nf.setVertexCorrelations(debug=True)\n",
    "    nf.setPointsOfInterest(debug=True)\n",
    "    nf.setDayTimes(debug=True)\n",
    "    nf.setDwellTimes(debug=True)\n",
    "    nf.setDurations(debug=True)\n",
    "    nf.setITAs(debug=True)\n",
    "    nf.setEdgeFeatures(debug=True)\n",
    "    nf.setVertexFeatures(debug=True)\n",
    "    nf.setHomeVertexFeatures(debug=True)\n",
    "    nf.setEdgeFractions(debug=True)\n",
    "    nf.setVertexFractions(debug=True)\n",
    "    nf.setUniqueEdgeProperties(debug=True)\n",
    "    nf.setUniqueVertexProperties(debug=True)\n",
    "    df = nf.getFeatureDataFrame(debug=True)\n",
    "\n",
    "    nf = networkFeatures(dn)\n",
    "    nf.setGlobalNetworkFeatures(debug=True)\n",
    "    nf.setArticulationStructure(debug=True)\n",
    "    #nf.setCliqueStructure(debug=True)\n",
    "    nf.setCommunityStructure(debug=True)\n",
    "    nf.setDyadCensus(debug=True)\n",
    "    nf.setEdgeCorrelations(debug=True)\n",
    "    nf.setVertexCorrelations(debug=True)\n",
    "    nf.setPointsOfInterest(debug=True)\n",
    "    nf.setDayTimes(debug=True)\n",
    "    nf.setDwellTimes(debug=True)\n",
    "    nf.setDurations(debug=True)\n",
    "    nf.setITAs(debug=True)\n",
    "    nf.setEdgeFeatures(debug=True)\n",
    "    nf.setVertexFeatures(debug=True)\n",
    "    nf.setHomeVertexFeatures(debug=True)\n",
    "    nf.setEdgeFractions(debug=True)\n",
    "    nf.setVertexFractions(debug=True)\n",
    "    nf.setUniqueEdgeProperties(debug=True)\n",
    "    nf.setUniqueVertexProperties(debug=True)\n",
    "    df = nf.getFeatureDataFrame(debug=True)\n",
    "    #dn.getVertexAttributes(0, debug=True)\n",
    "    #dn.printEdges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Using These Right Now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buildings OSM Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spOSMBuildingsMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"buildingsOSM\"\n",
    "    spOSMBuildingsMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName, force=False)\n",
    "    if isSparkDataFrame(spOSMBuildingsMap):\n",
    "        print(\"   Columns = {0}\".format(spOSMBuildingsMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## College Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoCollegeMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"College\"\n",
    "    spGeoCollegeMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName)\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoAutoMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"Auto\"\n",
    "    spGeoAutoMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName)\n",
    "    if isSparkDataFrame(spGeoRailMap):\n",
    "        spGeoAutoMap = spGeoAutoMap.withColumnRenamed('Automatic_Traffic_Counters', 'TrafficCounters')\n",
    "        spGeoAutoMap = spGeoAutoMap.withColumnRenamed('Weigh_in_Motion_Stations', 'WeighStations')\n",
    "        print(\"   Columns = {0}\".format(spGeoAutoMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rail Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoRailMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"Rail\"\n",
    "    spGeoRailMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName)\n",
    "    if isSparkDataFrame(spGeoRailMap):\n",
    "        spGeoRailMap = spGeoRailMap.withColumnRenamed('RRBRIDGES', 'RailBridge')\n",
    "        spGeoRailMap = spGeoRailMap.withColumnRenamed('Rail_Points', 'RailPoint')\n",
    "        print(\"   Columns = {0}\".format(spGeoRailMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## School Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoSchoolMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"School\"\n",
    "    spGeoSchoolMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName)\n",
    "_, _ = clock(\"Last Run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Venue Spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spGeoVenueMap = None\n",
    "if not isLocal:\n",
    "    suffix = \"Venue\"\n",
    "    spGeoVenueMap = getGeohashSparkDataFrame(suffix, 7, hc, scsql, inputDBName)\n",
    "    if isSparkDataFrame(spGeoVenueMap):\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Arenas_NBA', 'ArenasNBA')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Arenas_NCAA_Div_1_Basketball', 'ArenasNCAA')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Arenas_NHL', 'ArenasNHL')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Arenas_WNBA', 'ArenasWNBA')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Raceways_NASCAR', 'RacewaysNASCAR')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Stadiums_MLB', 'StadiumsMLB')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Stadiums_NCAA_Div_1_Football', 'StadiumsNCAA')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Stadiums_NFL', 'StadiumsNFL')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('State_Fairgrounds', 'StateFairgrounds')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('Tracks_Horses', 'TracksHorses')\n",
    "        spGeoVenueMap = spGeoVenueMap.withColumnRenamed('TracksIRL', 'TracksIRL')\n",
    "        print(\"   Columns = {0}\".format(spGeoVenueMap.columns))\n",
    "_, _ = clock(\"Last Run\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CE_py36",
   "language": "python",
   "name": "ce_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
